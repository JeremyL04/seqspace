module SeqSpace

using GZip
using BSON: @save
using LinearAlgebra: norm, svd, Diagonal, dot
using Statistics: quantile, std
using Flux, Zygote
using ProgressMeter
using Roots: find_zero, rrule

import BSON

include("io.jl")
include("rank.jl")
include("model.jl")
include("radon.jl")
include("pointcloud.jl")
include("generate.jl")
include("normalize.jl")
include("manifold.jl")
include("infer.jl")
include("scrna.jl")

using .PointCloud, .DataIO, .SoftRank, .ML

export Result, HyperParams
export linearprojection, fitmodel, extendfit
export marshal, unmarshal
export version_info

# ------------------------------------------------------------------------
# globals

# ------------------------------------------------------------------------
# types

"""
    mutable struct HyperParams
        dâ‚’ :: Int
        Ws :: Array{Int,1}
        BN :: Array{Int,1}
        DO :: Array{Int,1}
        N  :: Int
        Î´  :: Int
        Î·  :: Float64
        B  :: Int
        V  :: Int
        k  :: Int
        Î³â‚“ :: Float32
        Î³áµ¤ :: Float32
        g  :: Function
    end

HyperParams is a collection of parameters that specify the network architecture and training hyperparameters of the autoencoder.
`dâ‚’` is the desired output dimensionality of the encoding layer
`Ws` is a collection of the network layer widths. The number of entries controls the depth. The decoder is mirror-symmetric.
`BN` is the collection of layers that will be _followed_ by batch normalization.
`DO` is the collection of layers that will be _followed_ by dropout.
`N`  is the number of epochs to train against
`Î´`  is the number of epochs that will be sampled for logging
`Î·`  is the learning rate
`B`  is the batch size
`V`  is the number of points to partition for validation purposes, i.e. won't be training against
`k`  is the number of neighbors to use to estimate geodesics
`Î³â‚“` is the prefactor of distance soft rank loss
`Î³áµ¤` is the prefactor of uniform density loss
`g ` is the metric given to latent space
"""
mutable struct HyperParams
    dâ‚’ :: Int          # output dimensionality
    Ws :: Array{Int,1} # (latent) layer widths
    BN :: Array{Int,1} # (latent) layers followed by batch normalization
    DO :: Array{Int,1} # (latent) layers followed by drop outs
    N  :: Int          # number of epochs to run
    Î´  :: Int          # epoch subsample factor for logging
    Î·  :: Float64      # learning rate
    B  :: Int          # batch size
    V  :: Int          # number of points to partition for validation
    k  :: Int          # number of neighbors to use to estimate geodesics
    Î³â‚“ :: Float32      # prefactor of distance soft rank loss
    Î³áµ¤ :: Float32      # prefactor of uniform density loss
    g  :: Function     # metric given to latent space
end

"""
    euclideanÂ²(x)

Generate the matrix of pairwise distances between vectors `x`, assuming the Euclidean metric.
`x` is assumed to be ``d \times N`` where ``d`` denotes the dimensionality of the vector and ``N`` denotes the number.
"""
const euclideanÂ²(x) = sum( (x[d,:]' .- x[d,:]).^2 for d in 1:size(x,1) )

"""
    cylindersÂ²(x)

Generate the matrix of pairwise distances between vectors `x`, assuming the points are distributed on a cylinder.
`x` is assumed to be ``d \times N`` where ``d`` denotes the dimensionality of the vector and ``N`` denotes the number.
The first coordinate of `x` is assumed to be the polar coordinate.
"""
function cylinderÂ²(x)
    c = cos.(Ï€.*(x[1,:]))
    s = sin.(Ï€.*(x[1,:]))

    return (c' .- c).^2 .+ (s' .- s).^2 .+ euclideanÂ²(x[2:end,:])
end

HyperParams(; dâ‚’=2, Ws=Int[], BN=Int[], DO=Int[], N=200, Î´=10, Î·=1e-3, B=64, V=1, k=12, Î³â‚“=1, Î³áµ¤=1e-1, g=euclideanÂ²) = HyperParams(dâ‚’, Ws, BN, DO, N, Î´, Î·, B, V, k, Î³â‚“, Î³áµ¤, g)

"""
    struct Result
        param :: HyperParams
        loss  :: NamedTuple{(:train, :valid), Tuple{Array{Float64,1},Array{Float64,1}} }
        model
    end

Store the output of a trained autoencoder.
`param` stores the input hyperparameters used to design and train the neural network.
`loss` traces the dynamics of the optimization found during training.
`model` represents the learned pullback and pushforward functions.
"""
struct Result
    param :: HyperParams
    loss  :: NamedTuple{(:train, :valid), Tuple{Array{Float64,1},Array{Float64,1}} }
    info :: NamedTuple{(:ð•ƒáµ£, :ð•ƒâ‚“, :ð•ƒáµ¤, :history), Tuple{Array{Float64,1},Array{Float64,1},Array{Float64,1},Any}}
    model
end

"""
    marshal(r::Result)

Serialize a trained autoencoder to binary format suitable for disk storage.
Store parameters of model as contiguous array
"""
function marshal(r::Result)
    # activation functions TODO: should this be a hyperparameter?
    Ïƒ_interior = r.model.pullback.layers[3].Ïƒ
    Ïƒ_exterior = r.model.pullback.layers[end].Ïƒ
    
    # trainable parameters
    Ï‰â‚ = r.model.pullback    |> cpu |> Flux.params |> collect
    Ï‰â‚‚ = r.model.pushforward |> cpu |> Flux.params |> collect

    # untrainables
    Î²â‚ = [ (Î¼=layer.Î¼,ÏƒÂ²=layer.ÏƒÂ²) for layer in r.model.pullback.layers    if isa(layer,Flux.BatchNorm) ]
    Î²â‚‚ = [ (Î¼=layer.Î¼,ÏƒÂ²=layer.ÏƒÂ²) for layer in r.model.pushforward.layers if isa(layer,Flux.BatchNorm) ]

    return Result(r.param,r.loss,r.info,
                    (
                        pullback=(
                            params=Ï‰â‚,
                            batchs=Î²â‚,
                        ),
                        pushforward=(
                            params=Ï‰â‚‚,
                            batchs=Î²â‚‚,
                        ),
                        size=size(r.model.pullback.layers[1].weight,2)
                    )
                ),
                Ïƒ_interior,
                Ïƒ_exterior
end

"""
    unmarshal(r::Result)

Deserialize a trained autoencoder from binary format to semantic format.
Represents model as a collection of functors.
"""
function unmarshal(R)
    r = R[1] # This is a really ugly fix to this
    autoencoder = model(r.model.size, r.param.dâ‚’;
          Ws         = r.param.Ws,
          normalizes = r.param.BN,
          dropouts   = r.param.DO,
          interior_activation = R[2],
          exterior_activation = R[3]
    )

    Flux.loadparams!(autoencoder.pullback,    r.model.pullback.params)
    Flux.loadparams!(autoencoder.pushforward, r.model.pushforward.params)
    Flux.trainmode!(autoencoder.identity, false)

    i = 1
    for layer in autoencoder.pullback.layers
        if isa(layer, Flux.BatchNorm)
            layer.Î¼  = r.model.pullback.batchs[i].Î¼
            layer.ÏƒÂ² = r.model.pullback.batchs[i].ÏƒÂ²
            i += 1
        end
    end

    i = 1
    for layer in autoencoder.pushforward.layers
        if isa(layer, Flux.BatchNorm)
            layer.Î¼  = r.model.pushforward.batchs[i].Î¼
            layer.ÏƒÂ² = r.model.pushforward.batchs[i].ÏƒÂ²
            i += 1
        end
    end

    param = HyperParams(;
        dâ‚’ = r.param.dâ‚’,
        Ws = r.param.Ws,
        BN = r.param.BN,
        DO = r.param.DO,
        N  = r.param.N,
        Î´  = r.param.Î´,
        Î·  = r.param.Î·,
        B  = r.param.B,
        V  = r.param.V,
        k  = r.param.k,
        Î³â‚“ = r.param.Î³â‚“,
        Î³áµ¤ = r.param.Î³áµ¤,
    )

    return Result(param, r.loss, r.info, autoencoder)
end

# ------------------------------------------------------------------------
# utility functions

# Î± := rescale data by
# Î´ := subsample data by
function pointcloud(;Î±=1, Î´=1)
    verts, _ = open("$root/gut/mesh_apical_stab_000153.ply") do io
        read_ply(io)
    end

    return Î±*vcat(
        map(v->v.x, verts)',
        map(v->v.y, verts)',
        map(v->v.z, verts)'
    )[:,1:Î´:end]
end

function expression(;raw=false)
    scrna, genes, _  = if raw
        GZip.open("$root/dvex/dge_raw.txt.gz") do io
            read_matrix(io; named_cols=false, named_rows=true)
        end
    else
        GZip.open("$root/dvex/dge_normalized.txt.gz") do io
            read_matrix(io; named_cols=true, named_rows=true)
        end
    end

    return scrna, genes
end

# ------------------------------------------------------------------------
# i/o

# assumes a BSON i/o
function load(io::IO)
    database = BSON.parse(io)
    result, input = database[:result], database[:in]
end

mean(x) = length(x) > 0 ? sum(x) / length(x) : 0
mean(x::Matrix;dims=1) = sum(x;dims=dims) / size(x,dims)

function cor(x, y)
    Î¼ = (
        x=mean(x),
        y=mean(y)
    )
    var = (
       x=mean(x.^2) .- Î¼.x^2,
       y=mean(y.^2) .- Î¼.y^2
    )

    return (mean(x.*y) .- Î¼.x.*Î¼.y) / sqrt(var.x*var.y)
end

"""
    buildloss(model, DÂ², param)

Return a loss function used to train a neural network `model` according to input hyperparameters `param`.
`model` is a object with three fields, `pullback`, `pushforward`, and `identity`.
`pullback` and `pushforward` refers to the encoder and decoder layers respectively, while the identity is the composition.
`DÂ²` is a matrix of pairwise distances that will be used as a quenched hyperparameter in the distance soft rank loss.
"""
function buildloss(model, DÂ², param)
    return function(x, i::T, output::Bool) where T <: AbstractArray{Int,1}
        z = model.pullback(x)
        y = model.pushforward(z)

        # reconstruction loss
        Ïµáµ£ = sum(sum((x.-y).^2, dims=2)) / sum(sum(x.^2,dims=2))

        # distance softranks
        DzÂ² = param.g(z)
        DxÂ² = DÂ²[i,i]

        dx, dz = PointCloud.upper_tri(DxÂ²), PointCloud.upper_tri(DzÂ²)
        rx, rz = softrank(dx ./ mean(dx)), softrank(dz ./ mean(dz))
        Ïµâ‚“ = 1 - cor(1 .- rx, 1 .- rz)

        Ïµáµ¤ = let
            N = size(z,2)
            Nâ‚› = 10 # Number of slices
            Î˜ = (0+0.001:Ï€/(Nâ‚›):Ï€-0.01)
            proj = [[dot( point , [cos(Î¸),sin(Î¸)] ) for point âˆˆ eachcol(z)] for Î¸ âˆˆ Î˜]
            Y = collect(0:1/(N-1):1)
            InvCDFs = [[Radon.Square_InvCDFRadon(y,Î¸) for y âˆˆ Y] for Î¸ âˆˆ Î˜]
            mean([mean( (InvCDFs[i] .- sort(proj[i])).^2 ) for i âˆˆ 1:length(Î˜)].^(1/2))
        end

        if output
            println(stderr, "Ïµáµ£=$(Ïµáµ£), Ïµâ‚“=$(Ïµâ‚“), Ïµáµ¤=$(Ïµáµ¤)")
        end

        return Ïµáµ£ + param.Î³â‚“*Ïµâ‚“ + param.Î³áµ¤*Ïµáµ¤
    end
end

function build_data_loss(model, DÂ², param)
    return function(x, i::T, output::Bool) where T <: AbstractArray{Int,1}
        z = model.pullback(x)
        y = model.pushforward(z)

        # reconstruction loss
        Ïµáµ£ = sum(sum((x.-y).^2, dims=2)) / sum(sum(x.^2,dims=2))

        # distance softranks
        DzÂ² = param.g(z)
        DxÂ² = DÂ²[i,i]

        dx, dz = PointCloud.upper_tri(DxÂ²), PointCloud.upper_tri(DzÂ²)
        rx, rz = softrank(dx ./ mean(dx)), softrank(dz ./ mean(dz))
        Ïµâ‚“ = 1 - cor(1 .- rx, 1 .- rz)


        Ïµáµ¤ = let
            N = size(z,2)
            Nâ‚› = 10 # Number of slices
            Î˜ = (0+0.001:Ï€/(Nâ‚›):Ï€-0.01)
            proj = [[dot( point , [cos(Î¸),sin(Î¸)] ) for point âˆˆ eachcol(z)] for Î¸ âˆˆ Î˜]
            Y = collect(0:1/(N-1):1)
            InvCDFs = [[Radon.Square_InvCDFRadon(y,Î¸) for y âˆˆ Y] for Î¸ âˆˆ Î˜]
            mean([mean( (InvCDFs[i] .- sort(proj[i])).^2 ) for i âˆˆ 1:length(Î˜)].^(1/2))
        end

        if output
            println(stderr, "Ïµáµ£=$(Ïµáµ£), Ïµâ‚“=$(Ïµâ‚“), Ïµáµ¤=$(Ïµáµ¤)")
        end

        return Ïµáµ£,Ïµâ‚“,Ïµáµ¤
    end
end

# ------------------------------------------------------------------------
# main functions

"""
    linearprojection(x, d; Î”=1, Î›=nothing)

Project an empirical distance matrix `x` onto `d` top principal components.
Centers the result to have zero mean.
Returns the projection, as well as a function to transform a projected vector back to the embedding space.
Ignores top `Î”` principal components.
If `Î›` is not nothing, assumes it is a precomputed SVD decomposition.
"""
function linearprojection(x, d; Î”=1, Î›=nothing)
    Î› = isnothing(Î›) ? svd(x) : Î›

    Î¹ = (1:d) .+ Î”
    Ïˆ = Diagonal(Î›.S[Î¹])*Î›.Vt[Î¹,:]
    Î¼ = mean(Ïˆ, dims=2)

    xâ‚€ = (Î” > 0) ? Î›.U[:,1:Î”]*Diagonal(Î›.S[1:Î”])*Î›.Vt[1:Î”,:] : 0

    embed(x)   = (xâ‚€ .+ (Î›.U[:,Î¹]*(x.+Î¼)))
    embed(x,i) = (xâ‚€ .+ (Î›.U[:,Î¹]*(x.+Î¼[i])))

    return (
        embed      = embed,
        projection = (Ïˆ .- Î¼),
    )
end

"""
    fitmodel(data, param; DÂ²=nothing, dev=true, interior_activation=elu, exterior_activation=tanh_fast)

Train an autoencoder model, specified with `param` hyperparams, to fit `data`.
`data` is assumed to be sized ``d \times N`` where ``d`` and ``N`` are dimensionality and cardinality respectively.
If not nothing, `DÂ²` is assumed to be a precomputed distance matrix of point cloud `data`.
If `dev` is true, function will print to `stdout` and values of compoents of the loss (Ïµâ‚“,Ïµáµ£,Ïµáµ¤) will be recorded.
Returns a `Result` type.
"""
function fitmodel(
    data,
    param;
    DÂ²=nothing,
    dev=true,
    interior_activation=celu,
    exterior_activation=tanh_fast
)
    DÂ² = isnothing(DÂ²) ? geodesics(data, param.k).^2 : DÂ²
    if dev
        println(stderr, "done computing geodesics...")
    end

    M = model(size(data,1), param.dâ‚’;
          Ws         = param.Ws,
          normalizes = param.BN,
          dropouts   = param.DO,
          interior_activation = interior_activation,
          exterior_activation = exterior_activation,
    )

    nvalid = size(data,2) - ((size(data,2)Ã·param.B)-param.V)*param.B
    batch, index = validate(data, nvalid)

    loss = buildloss(M, DÂ², param)
    data_loss = build_data_loss(M, DÂ², param)
    E    = (
        train = Float64[],
        valid = Float64[],
    )
    Info = (
        ð•ƒáµ£ = Float64[],
        ð•ƒâ‚“ = Float64[],
        ð•ƒáµ¤ = Float64[],
        history = [] #TODO figure out the data struct of history and add it here 
    )

    progress = Progress(Int(round(param.N/10)); desc=">training model (1% â‰ˆ $(Int(round(param.N/10))) Epochs)", output=stderr)
    log = (n) -> begin
        if (n-1) % param.Î´ == 0
            push!(E.train, loss(batch.train, index.train, dev))
            push!(E.valid, loss(batch.valid, index.valid, dev))
            if dev
                push!(Info.ð•ƒáµ£, data_loss(batch.train, index.train, dev)[1])
                push!(Info.ð•ƒâ‚“, data_loss(batch.train, index.train, dev)[2])
                push!(Info.ð•ƒáµ¤, data_loss(batch.train, index.train, dev)[3])
            end
        end

        if (n-1) % 10 == 0
            next!(progress)
        end
        
        if dev
            push!(Info.history, M.pullback(data))
        end
        nothing
    end

    Flux.trainmode!(M.identity, true)
    train!(M, batch.train, index.train, loss;
        Î·   = param.Î·,
        B   = param.B,
        N   = param.N,
        log = log
    )
    Flux.trainmode!(M.identity, false)

    #Reset the progress bar for re-training
    progress = Progress(Int(round(param.N/10)); desc=">training model (1% â‰ˆ $(Int(round(param.N/10))) Epochs)", output=stderr)
    
    return Result(param, E, Info, M), (
        batch=batch,
        index=index,
        DÂ²=DÂ²,
        log=log,
        interior_activation=interior_activation,
        exterior_activation=exterior_activation
    )
end

"""
    extendfit(result::Result, input, epochs)

Retrain model within `result` on `input` data for `epochs` more iterations.
Returns a new `Result`.
"""
function extendfit(result::Result, input, new_params, epochs)
    loss = buildloss(result.model, input.DÂ², new_params)
    data_loss = build_data_loss(result.model, input.DÂ², new_params)

    train!(result.model, input.batch.train, input.index.train, loss; 
        Î·   = result.param.Î·,
        B   = result.param.B,
        N   = epochs,
        log = input.log
    )

    return result, input
end

function version_info()
    return "6/28 Version 1"
end

end
