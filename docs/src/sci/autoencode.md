# Manifold learning

## Introduction

In section [scRNAseq spatial inference](@ref), we demonstrated that scRNAseq expression data can be directly mapped to space using a reference database  of gene expression patterns, provided by the Berkeley Drosophila Transcriptional Network Project.
These results motivate a more ambitious question: is positional information directly encoded within gene expression and, as such, can we utilize _just_ scRNAseq counts to infer cellular spatial position _de-novo_?

We anchor our inference approach on the _continuum ansatz_ of cellular states in a developing embryo.
Specifically, we postulate that gene expression is a _smoothly_ varying, continuous function of space, and potentially other continuous degrees of freedom such as time.
Said another way, neighboring cells are assumed to be infinitesimally "close" in expression.
This formulation is a departure from the conventional, _discrete_, view of cellular fates taken, for example, in the description of the striped gene patterning observed during the Anterior-Posterior segmentation of the _Drosophila melongaster_ embryo.
From our viewpoint, a "discontinuity" of gene expression in a handful of components can also be explained by the curvature of a _smooth_ surface embedded in very high dimensions.

As formulated, spatial inference from scRNAseq data is equivalent to non-linear dimensional reduction: we want to find a small number of degrees of freedom that parameterize the variation across ``\sim 10^4`` genes.
From [Drosophila results](@ref), we know that at least minimally, we can describe gene expression within early Drosophila embryogenesis by ``31`` relevant components.
In this section, we analyze the data within this subspace to show that the data is ultimately generated by an even smaller set of degrees of freedom.
Furthermore, we outline a protocol to "learn" this parameterization and show that it can recapitulate space.

## Empirical analysis

### Hausdorff dimension estimation

A critical parameter to determine _empirically_ is the underlying dimensionality of gene expression of early _Drosophila embryogenesis, as given by the scRNAseq data.
Informally, we expect that if the data is sampled from an underlying ``d`` dimensional manifold, than the number of points ``n`` enclosed by a sphere of radius ``R`` should grow as
```math
    n(R) \sim R^d
```
However, we note that performing this comparison utilizing the Euclidean metric between normalized cellular expression would be manifestly incorrect; we postulated that gene expression is a manifold which implies a Euclidean metric _locally_ between neighbors.
There is no _a priori_ reason to expect Euclidean distances to be a good measure for far cells.
Similar considerations hold for other postulated global metrics.
Instead, we estimate geodesic distances empirically, in analog to the Isomap algorithm [^1].

[^1]: [A Global Geometric Framework for Nonlinear Dimensionality Reduction](https://www.science.org/doi/10.1126/science.290.5500.2319)

#### Neighborhood graph construction
The basic construction of the neighborhood graph is demonstrated in the below cartoon.
```@raw html
<p align="center">
<img src="/assets/drosophila/radius_scaling_neighborhood.svg" width="32%" class="center"/>
<img src="/assets/drosophila/radius_scaling_shortest_path.svg" width="32%" class="center"/>
<img src="/assets/drosophila/radius_scaling_ball.svg" width="32%" class="center"/>
</p>
```
Our algorithm to construct the neighborhood graph proceeds in three main steps.
First, we visit the local neighborhood of each point (cell) within our dataset.
In principle, the neighborhood can be defined by either a fixed number of neighbors ``k`` or a given radius ``R``.
In practice, for _Drosophila melongaster_ we chose to define neighborhood by ``k=8``.
The neighborhood is assumed to be a good approximation to the manifold tangent space and thus pairwise distances between the point and its neighbors are computed within the euclidean metric.
Each pairwise neighborhood relationship is captured by an edge, weighted by the pairwise distance.
The collection of all cells, and their neighborhood edges, constitute the _neighborhood graph_.

Given a neighborhood graph as defined above, computing the "geodesic" distance between cells reduces to finding the shortest path between all pairs of points.
This can be solved using either the Floyd-Warshall algorithm in ``\mathcal{O}(N^3)`` time [^2] or iteratively using Dijikstra's algorithm in ``\mathcal{O}(N(N\logN + E))`` time [^3].
We chose the later due to the sparsity of the neighborhood graph.
The estimation of geodesic distances, if performed as described, asymptotically approaches the true value as the density of points increases [^4].

[^2]: [Algorithm 97: shortest path](https://dl.acm.org/doi/pdf/10.1145/367766.368168)
[^3]: [A note on two problems in connexion with graphs](https://link.springer.com/article/10.1007/BF01386390)
[^4]: [Graph approximations to geodesics on embedded manifolds](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.359.7343&rep=rep1&type=pdf)

Given our estimate for all pairwise geodesic distances, we can now empirically estimate the _Hausdorff_ dimension.
Simply stated, the estimate is done by analyzing the scaling relationship between number of points enclosed within a _geodesic_ ball of radius ``R`` and the radius ``R`` itself.
Below we show the results plotted for each cell within our dataset, along with a trend line ``n(R) \sim R^3`` shown in red.
```@raw html
<p align="center">
<img src="/assets/drosophila/pointcloud_scaling.png" width="68%" class="center"/>
</p>
```
The manifold clearly looks three-dimensional.

### Physical proximity implies expression proximity

In [Neighborhood graph construction](@ref), we demonstrated that the normalized scRNAseq appears to be three-dimensional.
This supports _half_ of our continuum ansatz: gene expression of early Drosophila embryogenesis is consistent with being distributed along a low-dimensional manifold.
However, _a priori_ is is unclear that such dimensions meaningfully correspond to space.
Additionally, at the developmental stage sampled, the _Drosophila_ embryo is a two-dimensional epithelial monolayer and as such it is unclear what a potential third dimension would parameterize.
To this end, we utilize the positional labels calculated in [scRNAseq spatial inference](@ref) to assay if physically close cells are close along the putative manifold.
```@raw html
<p align="center">
<img src="/assets/drosophila/pointcloud_locality.png" width="68%" class="center"/>
</p>
```
Specifically, we demonstrate that conditioning on varying _average_ spatial geodesic distances results in quantitative effects in the distribution of pairwise expression geodesic distances.
Physically close cells are not only close in expression, but the median of the distribution increases quantitatively (albeit non-linearly) with increasing conditioned shells of distance.
Taken together, the scRNAseq data is empirically _consistent_ with our continuum expression ansatz.

### Isomap dimensional reduction
We provide additional evidence in support of our underlying hypothesis by utilizing the Isomap algorithm [^1].
As such, we simply perform a Principal Coordinates Analysis on the geodesic distances estimated in [Neighborhood graph construction](@ref).
This is tantamount to finding a set of low-dimensional coordinates ``\xi_\alpha`` that minimize the strain between their Euclidean embedding and the given distance matrix ``D_{\alpha\beta}``.
```math
    E(\{\xi_\alpha\}) = \left(\frac{\left(\sum_{\alpha,beta} D_{\alpha\beta} - |\xi_\alpha - \xi_\beta|^2\right)^2}{\sum_{\alpha,\beta} D_{\alpha\beta}^2 }\right)^{1/2}
```
```@raw html
<p align="center">
<img src="/assets/drosophila/isomap_AP.png" width="42%" class="center"/>
<img src="/assets/drosophila/isomap_DV.png" width="42%" class="center"/>
</p>
```
Above, we show the resultant embedding, colored by the estimated AP (anterior-posterior) and DV (dorsal-vental) position, obtained by averaging over the distribution obtained in [scRNAseq spatial inference](@ref).
It is clear that two of the major axes of the embedding quantitatively segregate both spatial axes of the embryo.
We emphasize that the estimated spatial positions _were not_ used to generate the shown embedding, but rather _only_ the underlying neighborhood distances of cells within our scRNAseq dataset.
This is taken as strong evidence in support of our underlying hypothesis.

By varying the dimensionality of the Isomap embedding, we see that three dimensions is the "knee" of dimensioning returns, consistent with the 3-dimensional scaling observed above.
```@raw html
<p align="center">
<img src="/assets/drosophila/isomap_dimension.png" width="68%" class="center"/>
</p>
```

## Nonlinear manifold learning

Want:
* Dimensional reduction
* Nonlinear
* Differentiable
* Generalizable
* Unsupervised

Natural choice for an autoencoder.
Utilize known positional labels as a validation step.
Not used for training purposes.

### Network architecture

How to pick depth? Width?
Worry about overfitting: enter dropout and batch normalization.
Vanilla autoencoder: latent space is not readily interpretable.

### Topological conservation
In order to learn an interpretable latent space representation of the intrinsic gene expression manifold, we wish to constrain the estimated pullback to conserve the topology of the input scRNAseq data.
This immediately poses the question: what topological features do we wish to preserve from the data to latent space and how do we empirically measure them?
The answers immediately determine the additional terms one must add to the objective function used for training.

We opt to utilize an explicitly _geometric_ formalism that will implicitly constrain _topology_.
The intuition for this choice is guided by _Differential Geometry_: a metric tensor uniquely defines a distance function between any two points on a manifold; the topology induced by this distance function will always coincide with the original topology of the manifold.
Thus, by imposing preservation of pairwise distances in the latent space relative to the data, we implicitly conserve topology.
It is important to note that this assumes our original scRNAseq data is sampled from a metric space that we have access to.
We note that there have been recent promising attempts at designing loss functions parameterized by explicit topological invariants formulated by _Topological Data Analysis_, e.g. persistent homology.
Lastly, one could envision having each network layer operate on a simplicial complex, rather than a flat vector of real numbers, however it is unclear how to parameterize the feed-forward function.

Thus the first task is to formulate an algorithm to approximate the metric space the point cloud is sampled from and subsequently utilize our estimate to compute all pairwise distances.
Again we proceed guided by intuition gleaned from _Differential Geometry_: pairwise distances within local neighborhoods are expected to be well-described by a Euclidean metric in the tangent space.
Conversely, macroscopic distances can only be computed via integration against the underlying metric along the corresponding geodesic.
As such, we first estimate the local tangent space of our input data by computing pairwise distances within local neighborhoods around each point, either defined by a fixed radius or fixed number of neighbors.
This defines a sparse, undirected graph in which edges only exist within our estimated tangent spaces and are weighted by the euclidean distance within the embedded space.
The resultant neighborhood graph serves as the basis for many dimensional reduction algorithms, such as **Isomap**, **UMAP** and **tSNE**.
Pairwise distances between _any_ two points in the original dataset can then be found by simple graph traversal to find the shortest possible path between two graph vertices, the discrete analog of a continuum geodesic.
It has been shown that the distance estimated by this algorithm asymptotically approaches the true distance as the number of data points sampled increases.
We denote ``D_{\alpha\beta}`` as the resultant pairwise distances between cell ``\alpha,\beta``.

#### Isometric formulation
The most straightforward manner to preserve distances between the input data and the latent representation is to impose isometry, i.e. distances in both spaces quantitatively agree.
This would be achieved by supplementing the objective function with the term
```math
E_{iso} = \displaystyle\sum\limits_{\alpha,\beta} \left(D_{\alpha\beta} - \left|\left| \xi_\alpha - \xi_\beta \right|\right| \right)^2
```
Utilizing this term is problematic for several reasons:
1. Large distances dominate the energetics and as such large-scale features of the intrinsic manifold will be preferentially fit.
2. Generically, ``d`` dimensional manifolds can not be isometrically embedded into ``\mathbb{R}^d``, e.g. the sphere into the plane.
3. It trusts the computed distances quantitatively. We simply want close cells to be close in the resultant latent space.

#### Monometric formulation
Consider a vector ``\psi_\alpha`` of scores of length ``n`` we wish to rank.
Furthermore, define ``\sigma \in \Sigma_n`` to be an arbitrary permutation of ``n`` such scores.
We define the **argsort** to be the permutation that sorts ``\psi`` in descending order
```math
    \bar{\sigma}\left(\bm{\psi}\right) \equiv \left(\sigma_1\left(\bm{\psi}\right),...,\sigma_n\left(\bm{\psi}\right)\right) \qquad \text{such that} \qquad
    \psi_{\bar{\sigma}_1} \ge \psi_{\bar{\sigma}_1} \ge ... \ge \psi_{\bar{\sigma}_n}
```

The definition of the **sorted** vector of scores ``\bar{\bm{\psi}}_\alpha \equiv \psi_{\bar{\sigma}_\alpha}`` thus follows naturally.
Lastly, the **rank** of vector ``\bm{\psi}`` is defined as the inverse permutation of **argsort**.
```math
    R\left(\bm{\psi}\right) \equiv \bar{\sigma}^{-1}\left(\bm{\psi}\right)
```
We wish to devise an objective function that contains functions of the rank of some latent space variables.
However, ``R(\bm{\psi})`` is a non-differentiable function; it maps a vector in ``\mathbb{R}^n`` to a permutation of ``n`` items.
Hence, we can not directly utilize the rank in a loss function as there is no way to backpropagate gradient information to the network parameters.
In order to rectify this limitation, we first reformulate the ranking problem as a linear programming problem that permits efficient regularization.
Note, the presentation here follows closely the original paper [^5]

[^5]: [Fast Differentiable Sorting and Ranking](https://arxiv.org/abs/2002.08871)

##### Linear program formulation

The **sorting** and **ranking** problem can be formulated as discrete optimization over the set of n-permutations ``\Sigma_n``
```math
    \bar{\sigma}\left(\bm{\psi}\right) \equiv \underset{\bm{\sigma}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\sigma_\alpha} \rho_{\alpha}
```
```math
    R\left(\bm{\psi}\right)
    \equiv \bar{\sigma}\left(\bm{\psi}\right)^{-1} 
    \equiv \left[\underset{\bm{\sigma}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\sigma_\alpha} \rho_{\alpha} \right]^{-1}
    \equiv \left[\underset{\bm{\sigma^{-1}}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_{\alpha} \rho_{\sigma^{-1}_\alpha} \right]^{-1}
    \equiv \underset{\bm{\pi}\in\Sigma_n}{\mathrm{argmax}} \ \displaystyle\sum\limits_{\alpha} \psi_\alpha \rho_{\pi(\alpha)}
```
where ``\rho_\alpha \equiv \left(n, n-1, ..., 1\right)``
In order to regularize the problem, and thus allow for continuous optimization, we imagine the convex hull of all permutations induced by an arbitrary vector ``\bm{\omega} \in \mathbb{R}^n``.
```math
    \Omega\left(\bm{\omega}\right) \equiv \text{convhull}\left[\left\{\bm{\omega}_{\sigma_\alpha}: \sigma \in \Sigma_n \right\}\right] \subset \mathbb{R}^n
```
This is often referred to as the _permutahedron_ of ``\bm{\omega}``; it is a convex polytope in n-dimensions whose vertices are the permutations of ``\bm{\omega}``
It follows directly from the fundamental theorem of linear programming, that the solution will almost surely be achieved at the vertex.
Thus the above discrete formulation can be rewritten as an optimization over continuous vectors contained on the _permutahedron_
```math
    \bm{\psi}_{\bar{\sigma}\left(\bm{\psi}\right)} \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\psi}\right)}{\mathrm{argmax}} \ \bm{\omega}\cdot\bm{\rho}
    \qquad
    \bm{\rho}_{R\left(\bm{\psi}\right)} \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \ \bm{\psi}\cdot\bm{\omega}
```
Utilizing the fact that ``\rho_{R\left(\bm{\psi}\right)} = R\left(-\bm{\psi}\right)``
```math
    R\left(\bm{\psi}\right) \equiv -\underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \ \bm{\psi}\cdot\bm{\omega}
```
Unfortunately, since ``\bm{\psi}`` appears in the **rank** objective function, any small perturbation in ``\bm{\psi}`` can force the solution of the linear program to discontinuously transition to another vertex.
As such, in its current form, it is still not differentiable.
Note, this is not true for the sorted vector, it appears in the constraint polyhedron; it has a unique Jacobian and can be directly used in neural networks.
The only way to proceed is to introduce convex regularization.

##### Regularization
We revise our objective function by Euclidean projection and thus introduce quadratic regularization on the norm of the solution.
Specifically, we define the **soft rank** operators as the extrema of the objective function
```math
    \tilde{R}\left(\bm{\psi}\right) \equiv \underset{\bm{\omega}\in\Omega\left(\bm{\rho}\right)}{\mathrm{argmax}} \left[ -\bm{\psi}\cdot\bm{\omega}
    - \frac{\epsilon}{2}\left|\left|\omega\right|\right|^2 \right]
```
Note that the limit ``\epsilon \rightarrow 0`` reproduces the linear programming formulation of the rank operator introduced above.
Conversely, in the limit ``\epsilon \rightarrow \infty``, the solution will go to a constant vector that has the smallest modulus on the _permutahedron_.

##### Solution
It has been demonstrated before that the above problem reduces to simple isotonic regression[^5][^6].
Specifically,
```math
R\left(\bm{\psi}\right) = -\frac{\bm{\psi}}{\epsilon} -
    \left[\underset{\omega_1 \ge \omega_2 \ge ... \ge \omega_n}{\mathrm{argmin}}
    \frac{1}{2} \left|\left|\bm{\omega} + \bm{\rho} + \frac{\bm{\psi}}{\epsilon} \right|\right|^2\right]_{\sigma^{-1}(\bm{\psi})}
\equiv -\frac{\bm{\psi}}{\epsilon} - \tilde{\bm{\omega}}\left(\bm{\psi},\bm{\rho}\right)
```
Importantly, isotonic regression is well-studied and can be solved in linear time.
Furthermore, the solution admits a simple, calculatable Jacobian
```math
\partial_{\psi_\alpha} R_\beta\left(\bm{\psi}\right)
= \frac{-\delta_{\alpha\beta}}{\epsilon} - \partial_{\psi_\alpha}\tilde{\omega_\beta}\left(\bm{\psi},\bm{\rho}\right)
= \frac{-\delta_{\alpha\beta}}{\epsilon} - 
    \begin{pmatrix}
    \bm{B}_1 & \bm{0} & \bm{0} \\
    \bm{0}   & \ddots & \bm{0} \\
    \bm{0}   & \bm{0} & \bm{B}_m \\
    \end{pmatrix}_{\alpha\beta}
```
where ``\bm{B}_i`` denotes the matrix corresponding to the `i^{th}` block obtained during isotonic regression.
It is a constant matrix whose number of rows and columns equals the size of the block, and whose values all sum to 1.

[^6]: [SparseMAP: Differentiable Sparse Structured Inference](https://arxiv.org/abs/1802.04223)

#### Loss function

### Uniform sampling of latent space

### Swiss roll validation

## Results
